{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b87abc-fee5-4226-8ccf-5b53a1a87ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc9f7a-0cec-4564-96de-1b0e94d6c4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/04 23:22:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f2fce25-8f58-4a6e-a28f-859c5f3da9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f32d50a-9d4a-48de-9375-25f8cba7e6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bea730-5ab4-41d0-b82c-20701dd9af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz\"\n",
    "df = pd.read_csv(url, sep=',', index_col=0)\n",
    "\n",
    "df.to_csv(\"fhv_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07a7bd2-b067-47d0-8011-93735b3dcddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B00009</th>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00013</th>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00014</th>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00014</th>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B00014</th>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pickup_datetime     dropOff_datetime  PUlocationID  \\\n",
       "dispatching_base_num                                                           \n",
       "B00009                2019-10-01 00:23:00  2019-10-01 00:35:00         264.0   \n",
       "B00013                2019-10-01 00:11:29  2019-10-01 00:13:22         264.0   \n",
       "B00014                2019-10-01 00:11:43  2019-10-01 00:37:20         264.0   \n",
       "B00014                2019-10-01 00:56:29  2019-10-01 00:57:47         264.0   \n",
       "B00014                2019-10-01 00:23:09  2019-10-01 00:28:27         264.0   \n",
       "\n",
       "                      DOlocationID  SR_Flag Affiliated_base_number  \n",
       "dispatching_base_num                                                \n",
       "B00009                       264.0      NaN                 B00009  \n",
       "B00013                       264.0      NaN                 B00013  \n",
       "B00014                       264.0      NaN                 B00014  \n",
       "B00014                       264.0      NaN                 B00014  \n",
       "B00014                       264.0      NaN                 B00014  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4bdef68-6f8e-45f8-9d62-0ca3a1f0effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "603ccf26-8a4c-4fa9-97c2-687639df0551",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "types.StructField('PUlocationID', types.FloatType(), True),\n",
    "types.StructField('DOlocationID', types.FloatType(), True),\n",
    "types.StructField('SR_Flag', types.StringType(), True),\n",
    "types.StructField('Affiliated_base_number', types.StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "667e8587-c816-487c-b989-62aaaaffafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"fhv_tripdata_2019-10.csv\")\n",
    "spark_df = spark_df.withColumn(\"PUlocationID\", F.col(\"PUlocationID\").cast(types.IntegerType()))\n",
    "spark_df = spark_df.withColumn(\"DOlocationID\", F.col(\"DOlocationID\").cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f7b5e005-1939-429f-a21a-fffe1d954956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  null|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65ad6370-1427-4af9-aab3-948baab42656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PUlocationID', IntegerType(), True), StructField('DOlocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecb47fb4-80d2-449e-88a0-b304bd18072a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41c76413-0afb-4770-b6dd-71506b7eef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark_df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25c765f3-146e-4663-8bb3-cf93e835624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.write.parquet('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e03aa9a-f426-4e2c-899b-b2542c8a496d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 34M\n",
      "drwxr-xr-x 1 root root 4.0K Mar  5 05:44 ..\n",
      "drwxr-xr-x 2 root root 4.0K Mar  4 23:27 .\n",
      "-rw-r--r-- 1 root root    8 Mar  4 23:27 ._SUCCESS.crc\n",
      "-rw-r--r-- 1 root root    0 Mar  4 23:27 _SUCCESS\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00002-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00002-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00000-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00001-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00003-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00004-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root  44K Mar  4 23:27 .part-00005-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet.crc\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00000-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00001-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00003-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00004-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n",
      "-rw-r--r-- 1 root root 5.5M Mar  4 23:27 part-00005-327cbb13-c9ec-4a46-8f7d-7881294fbdcd-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lat -h ./fhvhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec835c9e-0ab1-4d1c-86cf-7ed6358d1510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e586d760-9a77-4170-84dc-31cfceab2bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: integer (nullable = true)\n",
      " |-- DOlocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad1ea761-fbe2-4a52-b6b2-14e013539f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|pickup_datetime|   dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+---------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00608|     2019-10-02|2019-10-02 23:58:13|        null|        null|   null|                B00608|\n",
      "|              B01730|     2019-10-02|2019-10-02 16:23:03|        null|        null|   null|                B02096|\n",
      "|              B00821|     2019-10-04|2019-10-04 15:53:08|        null|        null|   null|                B03021|\n",
      "|              B02920|     2019-10-03|2019-10-03 17:47:55|        null|        null|   null|                B01536|\n",
      "|              B01196|     2019-10-03|2019-10-03 07:00:54|        null|        null|   null|                B01196|\n",
      "|              B00256|     2019-10-01|2019-10-01 03:55:16|        null|        null|   null|                B00256|\n",
      "|              B01065|     2019-10-04|2019-10-04 09:13:27|        null|        null|   null|                B01065|\n",
      "|              B01553|     2019-10-03|2019-10-03 08:25:55|        null|        null|   null|                B01553|\n",
      "|              B01239|     2019-10-03|2019-10-03 13:40:38|        null|        null|   null|                B02875|\n",
      "|              B01239|     2019-10-03|2019-10-03 15:02:07|        null|        null|   null|                B01239|\n",
      "|              B01389|     2019-10-03|2019-10-03 11:20:58|        null|        null|   null|                B01389|\n",
      "|              B02677|     2019-10-02|2019-10-02 20:31:51|        null|        null|   null|                B02677|\n",
      "|              B03016|     2019-10-03|2019-10-03 05:29:10|        null|        null|   null|                B02882|\n",
      "|              B01315|     2019-10-03|2019-10-03 01:20:26|        null|        null|   null|                B01315|\n",
      "|              B02735|     2019-10-03|2019-10-03 10:32:38|        null|        null|   null|                B01233|\n",
      "|              B00860|     2019-10-03|2019-10-03 21:35:14|        null|        null|   null|                B00860|\n",
      "|              B01437|     2019-10-02|2019-10-02 08:32:59|        null|        null|   null|                B02880|\n",
      "|              B00987|     2019-10-04|2019-10-04 04:22:00|        null|        null|   null|                B00987|\n",
      "|              B00256|     2019-10-04|2019-10-04 14:06:29|        null|        null|   null|                B00256|\n",
      "|              B02111|     2019-10-01|2019-10-01 03:27:37|        null|        null|   null|                B02111|\n",
      "+--------------------+---------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "date_df = df.withColumn('pickup_datetime', F.to_date(df.pickup_datetime))\n",
    "date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a128737-ad76-4582-a48b-c3c23e2e4a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_df.select('*').filter(date_df.pickup_datetime == '2019-10-15').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f98aa58a-7373-4847-a190-7aa581e64573",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:====================================>                     (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|  631152|\n",
      "|  631152|\n",
      "|   87696|\n",
      "|   70128|\n",
      "|    8808|\n",
      "|    8784|\n",
      "|    1464|\n",
      "|    1056|\n",
      "|    1056|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "|     792|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "duration_df = spark_df.withColumn(\n",
    "    \"duration\", \n",
    "    (F.datediff(F.col(\"dropoff_datetime\"), F.col(\"pickup_datetime\")) * 24)\n",
    ")\n",
    "result_df = duration_df.select(\"duration\").orderBy(F.col(\"duration\").desc())\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2595e43a-16d3-450a-9cf5-af3fa23edde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.option(\"header\", \"true\").schema(schema).csv(\"fhv_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1f33e04-49e8-4e42-bdb0-e7bb5ed4f873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PUlocationID: short (nullable = true)\n",
      " |-- DOlocationID: short (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "810e9ec1-9379-41dc-97c6-bc166da5193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|pickup_datetime|dropOff_datetime|PUlocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "+--------------------+---------------+----------------+------------+------------+-------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.filter(spark_df.PUlocationID.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5fe21e7-4525-4713-a77d-4bcac36d4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.read_csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d54a2243-8947-42a6-94ba-6ace24356760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num            0\n",
       "pickup_datetime                 0\n",
       "dropOff_datetime                0\n",
       "PUlocationID                 8556\n",
       "DOlocationID                 8179\n",
       "SR_Flag                   1897493\n",
       "Affiliated_base_number      16955\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b73e6fd6-ad95-46fa-9c9c-00ea366af096",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv'\n",
    "zone_df = pd.read_csv(zone_url, sep=',', index_col=0)\n",
    "zone_df.to_csv('taxi_zone_loop.csv')\n",
    "zone_df = spark.read.csv('taxi_zone_loop.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62aea771-5689-4bd6-8137-941733d3950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(zone_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56b0f79e-1f7e-4fb7-a010-d7b0434542e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8f463da6-ed4e-4782-b84c-c5949da002e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0e01f52d-28bf-44b0-af4c-9ca2fafbd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+---+\n",
      "|PULocationID|       Zone|cnt|\n",
      "+------------+-----------+---+\n",
      "|           2|Jamaica Bay|  1|\n",
      "+------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df = spark_df.join(zone_df, F.col(\"PUlocationID\") == F.col(\"LocationID\"), 'left')\n",
    "grouped_df = joined_df.groupBy(F.col(\"PULocationID\"), F.col(\"Zone\")).agg(F.count(\"*\").alias(\"cnt\"))\n",
    "result_df = grouped_df.orderBy(F.col(\"cnt\")).limit(1)\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
